{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer模型微調\n",
    "\n",
    "本筆記本實作和微調Transformer模型：\n",
    "- DistilBERT: 輕量化BERT模型\n",
    "- RoBERTa: 強化版BERT模型\n",
    "\n",
    "比較不同Transformer架構的性能表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要套件\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 自定義模組\n",
    "from src.data import IMDBDataLoader, TextPreprocessor, IMDBDataset\n",
    "from src.models.transformers import (\n",
    "    DistilBERTClassifier, RoBERTaClassifier, \n",
    "    TransformerTrainer, TransformerModelManager\n",
    ")\n",
    "from src.evaluation.evaluator import ModelEvaluator\n",
    "from src.utils.logger import logger\n",
    "\n",
    "# 設定隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 檢查GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用設備: {device}\")\n",
    "print(\"套件載入完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入和預處理資料\n",
    "logger.info(\"載入IMDB資料集...\")\n",
    "\n",
    "data_loader = IMDBDataLoader(cache_dir=\"../data/raw\")\n",
    "train_texts, train_labels, test_texts, test_labels = data_loader.load_data()\n",
    "\n",
    "# 為了節省訓練時間，使用較小的資料子集\n",
    "# 在實際應用中可以使用完整資料集\n",
    "train_texts = train_texts[:5000]  # 使用5000個訓練樣本\n",
    "train_labels = train_labels[:5000]\n",
    "test_texts = test_texts[:1000]    # 使用1000個測試樣本\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "# 創建驗證集\n",
    "train_texts_final, val_texts, train_labels_final, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"訓練集: {len(train_texts_final)}\")\n",
    "print(f\"驗證集: {len(val_texts)}\")\n",
    "print(f\"測試集: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎文本預處理（保持原始結構供BERT使用）\n",
    "preprocessor = TextPreprocessor(\n",
    "    remove_html=True,\n",
    "    remove_urls=True,\n",
    "    lowercase=False,  # BERT對大小寫敏感\n",
    "    handle_negations=False,  # BERT能處理否定詞\n",
    "    remove_punctuation=False  # 保留標點符號\n",
    ")\n",
    "\n",
    "train_texts_clean = preprocessor.preprocess_batch(train_texts_final)\n",
    "val_texts_clean = preprocessor.preprocess_batch(val_texts)\n",
    "test_texts_clean = preprocessor.preprocess_batch(test_texts)\n",
    "\n",
    "print(\"文本預處理完成！\")\n",
    "print(f\"範例文本: {train_texts_clean[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建BERT格式的Dataset\n",
    "batch_size = 16  # Transformer模型通常使用較小的batch size\n",
    "max_length = 256\n",
    "\n",
    "# 創建Dataset\n",
    "train_dataset = IMDBDataset(\n",
    "    train_texts_clean, train_labels_final, \n",
    "    tokenizer_name='distilbert-base-uncased',\n",
    "    max_length=max_length, is_bert_like=True\n",
    ")\n",
    "\n",
    "val_dataset = IMDBDataset(\n",
    "    val_texts_clean, val_labels,\n",
    "    tokenizer_name='distilbert-base-uncased', \n",
    "    max_length=max_length, is_bert_like=True\n",
    ")\n",
    "\n",
    "test_dataset = IMDBDataset(\n",
    "    test_texts_clean, test_labels,\n",
    "    tokenizer_name='distilbert-base-uncased',\n",
    "    max_length=max_length, is_bert_like=True\n",
    ")\n",
    "\n",
    "# 創建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"訓練批次數: {len(train_loader)}\")\n",
    "print(f\"驗證批次數: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練DistilBERT模型\n",
    "logger.info(\"開始訓練DistilBERT模型...\")\n",
    "\n",
    "model_manager = TransformerModelManager(device=device)\n",
    "\n",
    "# 創建DistilBERT模型\n",
    "distilbert_model = model_manager.create_model(\n",
    "    'distilbert',\n",
    "    num_labels=2,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# 創建訓練器\n",
    "distilbert_trainer = TransformerTrainer(\n",
    "    model=distilbert_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    output_dir='../experiments/models'\n",
    ")\n",
    "\n",
    "# 設置優化器和調度器\n",
    "distilbert_trainer.setup_optimizer_and_scheduler(\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_epochs=3,\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "distilbert_history = distilbert_trainer.train(\n",
    "    epochs=3,\n",
    "    model_name='distilbert_imdb'\n",
    ")\n",
    "\n",
    "print(\"DistilBERT訓練完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製DistilBERT訓練歷史\n",
    "def plot_transformer_history(history, model_name):\n",
    "    \"\"\"繪製Transformer訓練歷史\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 損失曲線\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_title(f'{model_name} - Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 準確率曲線\n",
    "    axes[1].plot(history['val_accuracy'], label='Val Accuracy', marker='o', color='green')\n",
    "    axes[1].set_title(f'{model_name} - Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 學習率曲線\n",
    "    axes[2].plot(history['learning_rate'], label='Learning Rate', marker='d', color='red')\n",
    "    axes[2].set_title(f'{model_name} - Learning Rate')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 繪製DistilBERT訓練歷史\n",
    "plot_transformer_history(distilbert_history, 'DistilBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估DistilBERT模型\n",
    "logger.info(\"評估DistilBERT模型...\")\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# 載入最佳模型\n",
    "distilbert_trainer.load_model('../experiments/models/distilbert_imdb')\n",
    "\n",
    "# 在測試集上預測\n",
    "distilbert_predictions, distilbert_probs = distilbert_model.predict(test_texts_clean)\n",
    "\n",
    "# 評估結果\n",
    "distilbert_results = evaluator.evaluate_classification(\n",
    "    test_labels, distilbert_predictions, distilbert_probs, 'DistilBERT'\n",
    ")\n",
    "\n",
    "print(f\"DistilBERT測試準確率: {distilbert_results['accuracy']:.4f}\")\n",
    "print(f\"DistilBERT F1分數: {distilbert_results['f1_score']:.4f}\")\n",
    "\n",
    "if 'auc_roc' in distilbert_results:\n",
    "    print(f\"DistilBERT AUC-ROC: {distilbert_results['auc_roc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型性能總結\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRANSFORMER模型性能總結\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = evaluator.compare_models(evaluator.evaluation_results)\n",
    "print(comparison_df)\n",
    "\n",
    "# 繪製混淆矩陣\n",
    "if 'confusion_matrix' in distilbert_results:\n",
    "    fig = evaluator.plot_confusion_matrix(\n",
    "        distilbert_results['confusion_matrix'], 'DistilBERT'\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# 生成評估報告\n",
    "report = evaluator.generate_evaluation_report(evaluator.evaluation_results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例預測\n",
    "sample_texts = [\n",
    "    \"This movie was absolutely fantastic! Great acting and amazing plot.\",\n",
    "    \"Terrible movie, waste of time. Poor acting and boring story.\",\n",
    "    \"The film was okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "print(\"DistilBERT預測示例:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "predictions, probabilities = distilbert_model.predict(sample_texts)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    pred_label = \"正面\" if predictions[i] == 1 else \"負面\"\n",
    "    confidence = probabilities[i][predictions[i]]\n",
    "    \n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"預測: {pred_label} (信心度: {confidence:.3f})\")\n",
    "    print(f\"機率分佈: 負面={probabilities[i][0]:.3f}, 正面={probabilities[i][1]:.3f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}