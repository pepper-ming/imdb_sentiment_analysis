"""
FastAPIæ¨ç†æœå‹™

æä¾›RESTful APIæ¥å£é€²è¡Œæƒ…æ„Ÿåˆ†æé æ¸¬ï¼Œæ”¯æ´ï¼š
- å–®å€‹æ–‡æœ¬é æ¸¬
- æ‰¹æ¬¡æ–‡æœ¬é æ¸¬
- æ¨¡å‹åˆ‡æ›
- å¥åº·æª¢æŸ¥
- APIæ–‡æª”è‡ªå‹•ç”Ÿæˆ
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import uvicorn
import os
import time
from datetime import datetime

from .predictor import SentimentPredictor, ModelRegistry
from ..utils.logger import logger

# Pydanticæ¨¡å‹å®šç¾©
class PredictionRequest(BaseModel):
    """å–®å€‹é æ¸¬è«‹æ±‚"""
    text: str = Field(..., description="è¦åˆ†æçš„æ–‡æœ¬", min_length=1, max_length=10000)
    model_name: Optional[str] = Field(None, description="æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç¨±")

class BatchPredictionRequest(BaseModel):
    """æ‰¹æ¬¡é æ¸¬è«‹æ±‚"""
    texts: List[str] = Field(..., description="è¦åˆ†æçš„æ–‡æœ¬åˆ—è¡¨", min_items=1, max_items=100)
    model_name: Optional[str] = Field(None, description="æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç¨±")

class PredictionResponse(BaseModel):
    """é æ¸¬å›æ‡‰"""
    text: str
    sentiment: str = Field(..., description="æƒ…æ„Ÿæ¨™ç±¤: positive æˆ– negative")
    confidence: float = Field(..., description="é æ¸¬ä¿¡å¿ƒåº¦ (0-1)")
    probabilities: Dict[str, Optional[float]] = Field(..., description="å„é¡åˆ¥æ©Ÿç‡")
    inference_time: float = Field(..., description="æ¨ç†æ™‚é–“ (ç§’)")
    model_used: str = Field(..., description="ä½¿ç”¨çš„æ¨¡å‹åç¨±")

class BatchPredictionResponse(BaseModel):
    """æ‰¹æ¬¡é æ¸¬å›æ‡‰"""
    results: List[PredictionResponse]
    total_texts: int
    total_time: float
    average_time: float

class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    name: str
    type: str
    path: str
    is_loaded: bool
    parameters: Optional[Dict[str, Any]] = None

class HealthResponse(BaseModel):
    """å¥åº·æª¢æŸ¥å›æ‡‰"""
    status: str
    timestamp: str
    version: str
    available_models: List[str]
    current_model: Optional[str]
    uptime: float

# å‰µå»ºFastAPIæ‡‰ç”¨
app = FastAPI(
    title="IMDBæƒ…æ„Ÿåˆ†æAPI",
    description="åŸºæ–¼æ·±åº¦å­¸ç¿’çš„é›»å½±è©•è«–æƒ…æ„Ÿåˆ†ææœå‹™",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ CORSä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨åŸŸè®Šé‡
model_registry = None
current_predictor = None
start_time = time.time()

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶"""
    global model_registry, current_predictor
    
    logger.info("å•Ÿå‹•IMDBæƒ…æ„Ÿåˆ†æAPIæœå‹™...")
    
    # åˆå§‹åŒ–æ¨¡å‹è¨»å†Šè¡¨
    model_registry = ModelRegistry(models_dir="experiments/models")
    
    # è¨»å†Šé è¨­æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    _register_default_models()
    
    # è¼‰å…¥é è¨­æ¨¡å‹
    available_models = model_registry.get_available_models()
    if available_models:
        try:
            # å˜—è©¦è¼‰å…¥ç¬¬ä¸€å€‹å¯ç”¨æ¨¡å‹
            default_model = available_models[0]
            current_predictor = model_registry.load_model(default_model)
            logger.info(f"è¼‰å…¥é è¨­æ¨¡å‹: {default_model}")
        except Exception as e:
            logger.error(f"è¼‰å…¥é è¨­æ¨¡å‹å¤±æ•—: {e}")
    
    logger.info("APIæœå‹™å•Ÿå‹•å®Œæˆ")

def _register_default_models():
    """è¨»å†Šé è¨­æ¨¡å‹"""
    models_dir = "experiments/models"
    
    # è¨»å†Šå·²çŸ¥çš„æ¨¡å‹
    model_configs = [
        {
            "name": "distilbert_imdb",
            "path": f"{models_dir}/distilbert_imdb",
            "type": "transformer"
        },
        {
            "name": "logistic_regression_model",
            "path": f"{models_dir}/logistic_regression_model.joblib",
            "type": "sklearn"
        },
        {
            "name": "svm_model", 
            "path": f"{models_dir}/svm_model.joblib",
            "type": "sklearn"
        }
    ]
    
    for config in model_configs:
        if os.path.exists(config["path"]):
            model_registry.register_model(
                name=config["name"],
                model_path=config["path"],
                model_type=config["type"]
            )

@app.get("/", response_class=HTMLResponse)
async def root():
    """æ ¹è·¯å¾‘ï¼Œè¿”å›ç°¡å–®çš„HTMLä»‹é¢"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>IMDBæƒ…æ„Ÿåˆ†æAPI</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .container { max-width: 800px; margin: 0 auto; }
            .input-group { margin: 20px 0; }
            textarea { width: 100%; height: 100px; padding: 10px; }
            button { background: #007bff; color: white; padding: 10px 20px; border: none; cursor: pointer; }
            .result { margin: 20px 0; padding: 15px; background: #f8f9fa; border-radius: 5px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸ¬ IMDBæƒ…æ„Ÿåˆ†ææœå‹™</h1>
            <p>è¼¸å…¥é›»å½±è©•è«–æ–‡æœ¬ï¼ŒAIå°‡åˆ†æå…¶æƒ…æ„Ÿå‚¾å‘ã€‚</p>
            
            <div class="input-group">
                <label>è©•è«–æ–‡æœ¬:</label>
                <textarea id="textInput" placeholder="è¼¸å…¥æ‚¨çš„é›»å½±è©•è«–..."></textarea>
            </div>
            
            <button onclick="predict()">åˆ†ææƒ…æ„Ÿ</button>
            
            <div id="result" class="result" style="display:none;">
                <h3>åˆ†æçµæœ:</h3>
                <div id="resultContent"></div>
            </div>
            
            <hr>
            <p><a href="/docs">ğŸ“š APIæ–‡æª”</a> | <a href="/health">ğŸ” ç³»çµ±å¥åº·</a></p>
        </div>
        
        <script>
            async function predict() {
                const text = document.getElementById('textInput').value;
                if (!text.trim()) {
                    alert('è«‹è¼¸å…¥æ–‡æœ¬');
                    return;
                }
                
                const response = await fetch('/predict', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: text })
                });
                
                const result = await response.json();
                const resultDiv = document.getElementById('result');
                const resultContent = document.getElementById('resultContent');
                
                if (response.ok) {
                    const sentiment = result.sentiment === 'positive' ? 'ğŸ˜Š æ­£é¢' : 'ğŸ˜ è² é¢';
                    const confidence = (result.confidence * 100).toFixed(1);
                    
                    resultContent.innerHTML = `
                        <p><strong>æƒ…æ„Ÿ:</strong> ${sentiment}</p>
                        <p><strong>ä¿¡å¿ƒåº¦:</strong> ${confidence}%</p>
                        <p><strong>æ¨ç†æ™‚é–“:</strong> ${(result.inference_time * 1000).toFixed(1)}ms</p>
                    `;
                } else {
                    resultContent.innerHTML = `<p style="color:red;">éŒ¯èª¤: ${result.detail}</p>`;
                }
                
                resultDiv.style.display = 'block';
            }
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.post("/predict", response_model=PredictionResponse)
async def predict_sentiment(request: PredictionRequest):
    """
    å–®å€‹æ–‡æœ¬æƒ…æ„Ÿé æ¸¬
    
    Args:
        request: é æ¸¬è«‹æ±‚
        
    Returns:
        é æ¸¬çµæœ
    """
    if current_predictor is None:
        raise HTTPException(status_code=503, detail="æ²’æœ‰å¯ç”¨çš„æ¨¡å‹")
    
    try:
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œå˜—è©¦åˆ‡æ›
        if request.model_name:
            if request.model_name in model_registry.get_available_models():
                predictor = model_registry.load_model(request.model_name)
            else:
                raise HTTPException(status_code=400, detail=f"æ¨¡å‹ {request.model_name} ä¸å­˜åœ¨")
        else:
            predictor = current_predictor
        
        # åŸ·è¡Œé æ¸¬
        result = predictor.predict_single(request.text)
        
        return PredictionResponse(
            text=result['text'],
            sentiment=result['sentiment'],
            confidence=result['confidence'],
            probabilities=result['probabilities'],
            inference_time=result['inference_time'],
            model_used=model_registry.get_current_model() or "unknown"
        )
        
    except Exception as e:
        logger.error(f"é æ¸¬å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"é æ¸¬å¤±æ•—: {str(e)}")

@app.post("/predict/batch", response_model=BatchPredictionResponse)
async def predict_batch(request: BatchPredictionRequest):
    """
    æ‰¹æ¬¡æ–‡æœ¬æƒ…æ„Ÿé æ¸¬
    
    Args:
        request: æ‰¹æ¬¡é æ¸¬è«‹æ±‚
        
    Returns:
        æ‰¹æ¬¡é æ¸¬çµæœ
    """
    if current_predictor is None:
        raise HTTPException(status_code=503, detail="æ²’æœ‰å¯ç”¨çš„æ¨¡å‹")
    
    try:
        start_time = time.time()
        
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œå˜—è©¦åˆ‡æ›
        if request.model_name:
            if request.model_name in model_registry.get_available_models():
                predictor = model_registry.load_model(request.model_name)
            else:
                raise HTTPException(status_code=400, detail=f"æ¨¡å‹ {request.model_name} ä¸å­˜åœ¨")
        else:
            predictor = current_predictor
        
        # åŸ·è¡Œæ‰¹æ¬¡é æ¸¬
        results = predictor.predict_batch(request.texts)
        total_time = time.time() - start_time
        
        # æ ¼å¼åŒ–çµæœ
        prediction_responses = [
            PredictionResponse(
                text=result['text'],
                sentiment=result['sentiment'],
                confidence=result['confidence'],
                probabilities=result['probabilities'],
                inference_time=result['inference_time'],
                model_used=model_registry.get_current_model() or "unknown"
            )
            for result in results
        ]
        
        return BatchPredictionResponse(
            results=prediction_responses,
            total_texts=len(request.texts),
            total_time=total_time,
            average_time=total_time / len(request.texts)
        )
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡é æ¸¬å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹æ¬¡é æ¸¬å¤±æ•—: {str(e)}")

@app.get("/models", response_model=List[ModelInfo])
async def get_models():
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    if model_registry is None:
        return []
    
    models_info = []
    available_models = model_registry.get_available_models()
    current_model = model_registry.get_current_model()
    
    for model_name in available_models:
        model_info = model_registry.models[model_name]
        
        models_info.append(ModelInfo(
            name=model_name,
            type=model_info['type'],
            path=model_info['path'],
            is_loaded=model_info['predictor'] is not None,
            parameters=None  # å¯ä»¥æ·»åŠ æ¨¡å‹åƒæ•¸ä¿¡æ¯
        ))
    
    return models_info

@app.post("/models/{model_name}/load")
async def load_model(model_name: str):
    """è¼‰å…¥æŒ‡å®šæ¨¡å‹"""
    global current_predictor
    
    if model_registry is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹è¨»å†Šè¡¨æœªåˆå§‹åŒ–")
    
    if model_name not in model_registry.get_available_models():
        raise HTTPException(status_code=404, detail=f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
    
    try:
        current_predictor = model_registry.load_model(model_name)
        return {"message": f"æ¨¡å‹ {model_name} è¼‰å…¥æˆåŠŸ"}
        
    except Exception as e:
        logger.error(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {str(e)}")

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    uptime = time.time() - start_time
    
    return HealthResponse(
        status="healthy",
        timestamp=datetime.now().isoformat(),
        version="1.0.0",
        available_models=model_registry.get_available_models() if model_registry else [],
        current_model=model_registry.get_current_model() if model_registry else None,
        uptime=uptime
    )

if __name__ == "__main__":
    uvicorn.run(
        "src.inference.api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )