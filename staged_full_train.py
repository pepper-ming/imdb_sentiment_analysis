#!/usr/bin/env python3
"""
åˆ†éšæ®µå®Œæ•´IMDBæƒ…æ„Ÿåˆ†ææ¨¡å‹è¨“ç·´è…³æœ¬
ä½¿ç”¨å…¨éƒ¨è³‡æ–™é›†ï¼Œåˆ†éšæ®µåŸ·è¡Œä»¥ä¾¿ç›£æ§é€²åº¦
"""

import os
import sys
import time
import pickle
import warnings
import json
from pathlib import Path
from datetime import datetime
warnings.filterwarnings('ignore')

# æ·»åŠ srcåˆ°è·¯å¾‘
sys.path.append('src')

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix

def log_message(message):
    """è¨˜éŒ„å¸¶æ™‚é–“æˆ³çš„è¨Šæ¯"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {message}")
    
def load_data():
    """è¼‰å…¥å’Œæº–å‚™è³‡æ–™"""
    log_message("è¼‰å…¥é è™•ç†è³‡æ–™...")
    
    with open('data/processed/preprocessed_data.pkl', 'rb') as f:
        data = pickle.load(f)
    
    train_texts = data['train_texts']
    train_labels = data['train_labels']
    test_texts = data['test_texts']
    test_labels = data['test_labels']
    
    log_message(f"è³‡æ–™è¼‰å…¥å®Œæˆ:")
    log_message(f"  è¨“ç·´é›†: {len(train_texts):,} ç­†")
    log_message(f"  æ¸¬è©¦é›†: {len(test_texts):,} ç­†")
    
    return train_texts, train_labels, test_texts, test_labels

def create_tfidf_vectors(train_texts, test_texts):
    """å‰µå»ºTF-IDFå‘é‡"""
    log_message("é–‹å§‹TF-IDFå‘é‡åŒ–...")
    start_time = time.time()
    
    vectorizer = TfidfVectorizer(
        max_features=15000,    
        ngram_range=(1, 2),    
        min_df=3,              
        max_df=0.9,            
        sublinear_tf=True      
    )
    
    train_vectors = vectorizer.fit_transform(train_texts)
    test_vectors = vectorizer.transform(test_texts)
    
    vectorizer_time = time.time() - start_time
    
    log_message(f"TF-IDFå‘é‡åŒ–å®Œæˆ:")
    log_message(f"  è€—æ™‚: {vectorizer_time:.1f}ç§’")
    log_message(f"  ç‰¹å¾µç¶­åº¦: {train_vectors.shape[1]:,}")
    log_message(f"  è¨“ç·´å‘é‡å½¢ç‹€: {train_vectors.shape}")
    log_message(f"  æ¸¬è©¦å‘é‡å½¢ç‹€: {test_vectors.shape}")
    
    return train_vectors, test_vectors, vectorizer, vectorizer_time

def train_single_model(model, model_name, train_vectors, train_labels, test_vectors, test_labels):
    """è¨“ç·´å–®å€‹æ¨¡å‹ä¸¦è¿”å›çµæœ"""
    log_message(f"\né–‹å§‹è¨“ç·´ {model_name}...")
    
    # è¨“ç·´
    train_start = time.time()
    model.fit(train_vectors, train_labels)
    training_time = time.time() - train_start
    
    log_message(f"  {model_name} è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {training_time:.1f}ç§’")
    
    # é æ¸¬
    predict_start = time.time()
    train_predictions = model.predict(train_vectors)
    test_predictions = model.predict(test_vectors)
    prediction_time = time.time() - predict_start
    
    # è¨ˆç®—æŒ‡æ¨™
    train_accuracy = accuracy_score(train_labels, train_predictions)
    test_accuracy = accuracy_score(test_labels, test_predictions)
    test_f1 = f1_score(test_labels, test_predictions)
    
    # AUCè¨ˆç®—
    try:
        test_probabilities = model.predict_proba(test_vectors)[:, 1]
        test_auc = roc_auc_score(test_labels, test_probabilities)
    except:
        test_auc = 0.0
    
    log_message(f"  {model_name} çµæœ:")
    log_message(f"    è¨“ç·´æº–ç¢ºç‡: {train_accuracy:.4f}")
    log_message(f"    æ¸¬è©¦æº–ç¢ºç‡: {test_accuracy:.4f}")
    log_message(f"    æ¸¬è©¦F1åˆ†æ•¸: {test_f1:.4f}")
    log_message(f"    æ¸¬è©¦AUC: {test_auc:.4f}")
    
    return {
        'model_name': model_name,
        'training_time': training_time,
        'prediction_time': prediction_time,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'test_f1': test_f1,
        'test_auc': test_auc,
    }

def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    total_start_time = time.time()
    
    log_message("=== é–‹å§‹å®Œæ•´IMDBæƒ…æ„Ÿåˆ†ææ¨¡å‹è¨“ç·´ ===")
    
    # 1. è¼‰å…¥è³‡æ–™
    train_texts, train_labels, test_texts, test_labels = load_data()
    
    # 2. å‰µå»ºTF-IDFå‘é‡
    train_vectors, test_vectors, vectorizer, vectorizer_time = create_tfidf_vectors(train_texts, test_texts)
    
    # 3. å®šç¾©æ¨¡å‹ (æŒ‰è¨“ç·´é€Ÿåº¦é †åº)
    models_to_train = [
        ('naive_bayes', MultinomialNB(alpha=1.0), 'æ¨¸ç´ è²è‘‰æ–¯'),
        ('logistic_regression', LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'), 'é‚è¼¯å›æ­¸'),
        ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1), 'éš¨æ©Ÿæ£®æ—'),
        ('svm_linear', SVC(kernel='linear', random_state=42, probability=True, C=1.0), 'SVMç·šæ€§æ ¸')
    ]
    
    results = {}
    
    # 4. é€å€‹è¨“ç·´æ¨¡å‹
    for model_key, model, model_name in models_to_train:
        try:
            result = train_single_model(model, model_name, train_vectors, train_labels, test_vectors, test_labels)
            results[model_key] = result
            
            # å³æ™‚ä¿å­˜çµæœ
            intermediate_results = {
                'timestamp': datetime.now().isoformat(),
                'completed_models': list(results.keys()),
                'results': results
            }
            
            with open('experiments/results/training_progress.json', 'w', encoding='utf-8') as f:
                json.dump(intermediate_results, f, indent=2, ensure_ascii=False)
            
        except Exception as e:
            log_message(f"  éŒ¯èª¤: {model_name} è¨“ç·´å¤±æ•— - {str(e)}")
            continue
    
    total_time = time.time() - total_start_time
    
    # 5. ä¿å­˜å®Œæ•´çµæœ
    log_message("\n=== ä¿å­˜æœ€çµ‚çµæœ ===")
    
    final_results = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'total_time': total_time,
            'vectorizer_time': vectorizer_time,
            'dataset_info': {
                'train_samples': len(train_texts),
                'test_samples': len(test_texts),
                'feature_dim': train_vectors.shape[1]
            }
        },
        'model_results': results
    }
    
    # ä¿å­˜å®Œæ•´çµæœ
    with open('experiments/results/complete_training_results.pkl', 'wb') as f:
        pickle.dump(final_results, f)
    
    with open('experiments/results/complete_training_results.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    # 6. æ‰“å°æœ€çµ‚ç¸½çµ
    log_message("\n" + "="*70)
    log_message("å®Œæ•´è¨“ç·´çµæœç¸½çµ")
    log_message("="*70)
    log_message(f"ç¸½è¨“ç·´æ™‚é–“: {total_time:.1f}ç§’")
    log_message(f"è³‡æ–™é›†å¤§å°: {len(train_texts):,} è¨“ç·´ + {len(test_texts):,} æ¸¬è©¦")
    log_message(f"TF-IDFç‰¹å¾µ: {train_vectors.shape[1]:,} ç¶­")
    log_message("")
    log_message(f"{'æ¨¡å‹':<15} {'æ¸¬è©¦æº–ç¢ºç‡':<12} {'F1åˆ†æ•¸':<10} {'AUC':<8} {'è¨“ç·´æ™‚é–“':<10}")
    log_message("-"*65)
    
    # æŒ‰æº–ç¢ºç‡æ’åº
    if results:
        sorted_results = sorted(results.items(), key=lambda x: x[1]['test_accuracy'], reverse=True)
        
        for model_key, result in sorted_results:
            log_message(f"{result['model_name']:<15} {result['test_accuracy']:<12.4f} "
                       f"{result['test_f1']:<10.4f} {result['test_auc']:<8.4f} "
                       f"{result['training_time']:<10.1f}s")
        
        best_model = sorted_results[0][1]
        log_message(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model_name']} (æº–ç¢ºç‡: {best_model['test_accuracy']:.4f})")
    
    log_message(f"\nçµæœå·²ä¿å­˜è‡³ experiments/results/complete_training_results.json")
    log_message("=== å®Œæ•´è¨“ç·´å®Œæˆï¼ ===")
    
    return final_results

if __name__ == "__main__":
    try:
        # ç¢ºä¿çµæœç›®éŒ„å­˜åœ¨
        Path('experiments/results').mkdir(parents=True, exist_ok=True)
        
        results = main()
        
    except Exception as e:
        log_message(f"\nåš´é‡éŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()