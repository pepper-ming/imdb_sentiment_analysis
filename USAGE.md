# ğŸ“– ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æª”æä¾›IMDBæƒ…æ„Ÿåˆ†æå°ˆæ¡ˆçš„è©³ç´°ä½¿ç”¨æŒ‡å—ï¼ŒåŒ…æ‹¬å®‰è£ã€é…ç½®ã€è¨“ç·´å’Œéƒ¨ç½²ç­‰æ­¥é©Ÿã€‚

## ğŸ“‹ ç›®éŒ„

- [ç’°å¢ƒè¦æ±‚](#ç’°å¢ƒè¦æ±‚)
- [å®‰è£æ­¥é©Ÿ](#å®‰è£æ­¥é©Ÿ)
- [è³‡æ–™æº–å‚™](#è³‡æ–™æº–å‚™)
- [æ¨¡å‹è¨“ç·´](#æ¨¡å‹è¨“ç·´)
- [APIæœå‹™](#apiæœå‹™)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

## ğŸ”§ ç’°å¢ƒè¦æ±‚

### ç¡¬é«”è¦æ±‚
- **CPU**: 4æ ¸å¿ƒä»¥ä¸Šï¼ˆæ¨è–¦8æ ¸å¿ƒï¼‰
- **RAM**: 8GBä»¥ä¸Šï¼ˆæ¨è–¦16GBï¼‰
- **GPU**: NVIDIA GPU with 4GB+ VRAMï¼ˆå¯é¸ï¼Œç”¨æ–¼åŠ é€Ÿè¨“ç·´ï¼‰
- **å„²å­˜**: 10GBå¯ç”¨ç©ºé–“

### è»Ÿé«”è¦æ±‚
- **Python**: 3.8 - 3.11
- **æ“ä½œç³»çµ±**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **CUDA**: 11.8+ï¼ˆå¦‚ä½¿ç”¨GPUï¼‰

## ğŸ“¦ å®‰è£æ­¥é©Ÿ

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨pipï¼ˆæ¨è–¦ï¼‰

```bash
# 1. å…‹éš†å°ˆæ¡ˆ
git clone <repository-url>
cd imdb_sentiment_analysis

# 2. å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv .venv

# 3. å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
# Windows
.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate

# 4. å‡ç´špip
python -m pip install --upgrade pip

# 5. å®‰è£ä¾è³´
pip install -r requirements.txt

# 6. å®‰è£spaCyèªè¨€æ¨¡å‹
python -m spacy download en_core_web_sm

# 7. é©—è­‰å®‰è£
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"
python -c "import transformers; print(f'Transformersç‰ˆæœ¬: {transformers.__version__}')"
```

### æ–¹æ³•äºŒï¼šä½¿ç”¨conda

```bash
# 1. å»ºç«‹condaç’°å¢ƒ
conda create -n sentiment_analysis python=3.9
conda activate sentiment_analysis

# 2. å®‰è£PyTorchï¼ˆæ ¹æ“šæ‚¨çš„CUDAç‰ˆæœ¬ï¼‰
# CPUç‰ˆæœ¬
conda install pytorch torchvision torchaudio cpuonly -c pytorch
# GPUç‰ˆæœ¬ï¼ˆCUDA 11.8ï¼‰
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# 3. å®‰è£å…¶ä»–ä¾è³´
pip install transformers datasets scikit-learn pandas numpy matplotlib seaborn
pip install fastapi uvicorn nltk spacy beautifulsoup4 lime wandb

# 4. ä¸‹è¼‰èªè¨€æ¨¡å‹
python -m spacy download en_core_web_sm
```

## ğŸ“Š è³‡æ–™æº–å‚™

### è‡ªå‹•ä¸‹è¼‰IMDBè³‡æ–™é›†

```python
from src.data import IMDBDataLoader

# å‰µå»ºè³‡æ–™è¼‰å…¥å™¨
data_loader = IMDBDataLoader(cache_dir="data/raw")

# è‡ªå‹•ä¸‹è¼‰å’Œè¼‰å…¥è³‡æ–™
train_texts, train_labels, test_texts, test_labels = data_loader.load_data()

print(f"è¨“ç·´é›†: {len(train_texts)} ç­†")
print(f"æ¸¬è©¦é›†: {len(test_texts)} ç­†")
```

### æ‰‹å‹•è³‡æ–™æº–å‚™ï¼ˆå¯é¸ï¼‰

å¦‚æœè‡ªå‹•ä¸‹è¼‰å¤±æ•—ï¼Œå¯ä»¥æ‰‹å‹•æº–å‚™è³‡æ–™ï¼š

```bash
# 1. å»ºç«‹è³‡æ–™ç›®éŒ„
mkdir -p data/raw data/processed

# 2. ä¸‹è¼‰IMDBè³‡æ–™é›†
# è«‹å¾ https://ai.stanford.edu/~amaas/data/sentiment/ ä¸‹è¼‰
# æˆ–ä½¿ç”¨kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# 3. è§£å£“åˆ° data/raw/ ç›®éŒ„
```

## ğŸš€ æ¨¡å‹è¨“ç·´

### å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹

```bash
# å•Ÿå‹•Jupyter Notebook
jupyter lab notebooks/02_baseline_models.ipynb
```

æˆ–ä½¿ç”¨Pythonè…³æœ¬ï¼š

```python
from src.models import BaselineModelManager
from src.data import IMDBDataLoader, TextPreprocessor

# è¼‰å…¥å’Œé è™•ç†è³‡æ–™
data_loader = IMDBDataLoader()
train_texts, train_labels, test_texts, test_labels = data_loader.load_data()

preprocessor = TextPreprocessor(
    remove_html=True,
    lowercase=True,
    handle_negations=True
)
train_texts_clean = preprocessor.preprocess_batch(train_texts)

# è¨“ç·´æ¨¡å‹
model_manager = BaselineModelManager()
results = model_manager.train_all_models(train_texts_clean, train_labels)

# è©•ä¼°æ¨¡å‹
test_results = model_manager.evaluate_all_models(
    preprocessor.preprocess_batch(test_texts), 
    test_labels
)
```

### æ·±åº¦å­¸ç¿’æ¨¡å‹

```python
from src.models import TextCNN, BiLSTM, DeepLearningModelManager
from src.training import DeepLearningTrainer

# å»ºç«‹è©å½™è¡¨å’Œæ•¸æ“šè¼‰å…¥å™¨
# ... (è³‡æ–™æº–å‚™ä»£ç¢¼)

# å‰µå»ºæ¨¡å‹
model_manager = DeepLearningModelManager()
model = model_manager.create_model('textcnn', vocab_size=10000)

# è¨­ç½®è¨“ç·´å™¨
trainer = DeepLearningTrainer(model, train_loader, val_loader)
trainer.setup_optimizer_and_scheduler(learning_rate=1e-3)

# è¨“ç·´æ¨¡å‹
history = trainer.train(epochs=10, early_stopping_patience=3)
```

### Transformeræ¨¡å‹

```python
from src.models import DistilBERTClassifier, TransformerTrainer

# å‰µå»ºæ¨¡å‹
model = DistilBERTClassifier(num_labels=2)

# è¨­ç½®è¨“ç·´å™¨
trainer = TransformerTrainer(model, train_loader, val_loader)
trainer.setup_optimizer_and_scheduler(learning_rate=2e-5, num_epochs=3)

# è¨“ç·´æ¨¡å‹
history = trainer.train(epochs=3, model_name='distilbert_imdb')
```

## ğŸŒ APIæœå‹™

### å•Ÿå‹•æœå‹™

```bash
# åŸºæœ¬å•Ÿå‹•
python app.py

# æŒ‡å®šåŸ è™Ÿå’Œä¸»æ©Ÿ
uvicorn src.inference.api:app --host 0.0.0.0 --port 8080 --reload

# ç”Ÿç”¢ç’°å¢ƒå•Ÿå‹•
uvicorn src.inference.api:app --host 0.0.0.0 --port 8000 --workers 4
```

### APIç«¯é»èªªæ˜

#### 1. å¥åº·æª¢æŸ¥
```http
GET /health
```

#### 2. å–®å€‹é æ¸¬
```http
POST /predict
Content-Type: application/json

{
  "text": "This movie was fantastic!",
  "model_name": "distilbert_imdb"  // å¯é¸
}
```

#### 3. æ‰¹æ¬¡é æ¸¬
```http
POST /predict/batch
Content-Type: application/json

{
  "texts": ["Great movie!", "Terrible film"],
  "model_name": "distilbert_imdb"  // å¯é¸
}
```

#### 4. æ¨¡å‹ç®¡ç†
```http
GET /models                    // ç²å–æ¨¡å‹åˆ—è¡¨
POST /models/{model_name}/load // è¼‰å…¥æŒ‡å®šæ¨¡å‹
```

### APIå®¢æˆ¶ç«¯ç¯„ä¾‹

```python
import requests
import json

# APIåŸºç¤URL
BASE_URL = "http://localhost:8000"

# æ¸¬è©¦å¥åº·æª¢æŸ¥
response = requests.get(f"{BASE_URL}/health")
print(f"æœå‹™ç‹€æ…‹: {response.json()['status']}")

# å–®å€‹é æ¸¬
def predict_sentiment(text):
    response = requests.post(
        f"{BASE_URL}/predict",
        json={"text": text}
    )
    return response.json()

# æ‰¹æ¬¡é æ¸¬
def predict_batch(texts):
    response = requests.post(
        f"{BASE_URL}/predict/batch",
        json={"texts": texts}
    )
    return response.json()

# ä½¿ç”¨ç¯„ä¾‹
result = predict_sentiment("Amazing movie with great acting!")
print(f"é æ¸¬çµæœ: {result['sentiment']} (ä¿¡å¿ƒåº¦: {result['confidence']:.3f})")

batch_results = predict_batch([
    "Love this film!",
    "Worst movie ever",
    "It was okay"
])
for r in batch_results['results']:
    print(f"{r['text']} -> {r['sentiment']}")
```

## ğŸ”§ é€²éšé…ç½®

### è‡ªå®šç¾©é è™•ç†

```python
from src.data import TextPreprocessor

# è‡ªå®šç¾©é è™•ç†é…ç½®
preprocessor = TextPreprocessor(
    remove_html=True,
    remove_urls=True,
    lowercase=True,
    handle_negations=True,
    remove_stopwords=True,
    lemmatization=True,
    min_length=2
)

# è™•ç†æ–‡æœ¬
processed_text = preprocessor.preprocess("This movie was <b>amazing</b>!")
```

### æ¨¡å‹é…ç½®

```python
# æ·±åº¦å­¸ç¿’æ¨¡å‹é…ç½®
model_config = {
    'embed_dim': 128,
    'hidden_dim': 64,
    'num_layers': 2,
    'dropout_rate': 0.3
}

# Transformeræ¨¡å‹é…ç½®
transformer_config = {
    'model_name': 'distilbert-base-uncased',
    'num_labels': 2,
    'max_length': 256,
    'freeze_base': False
}
```

### è¨“ç·´é…ç½®

```python
# è¨“ç·´è¶…åƒæ•¸
training_config = {
    'learning_rate': 2e-5,
    'batch_size': 16,
    'epochs': 3,
    'weight_decay': 0.01,
    'warmup_ratio': 0.1,
    'early_stopping_patience': 3
}
```

## â“ å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼æ¨¡å‹è¨“ç·´å¾ˆæ…¢ï¼Ÿ
**A**: 
- ç¢ºä¿ä½¿ç”¨GPUåŠ é€Ÿï¼šæª¢æŸ¥CUDAå®‰è£å’ŒPyTorch GPUæ”¯æ´
- æ¸›å°‘batch_sizeï¼šå¦‚æœè¨˜æ†¶é«”ä¸è¶³
- ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹ï¼šå¦‚DistilBERTè€ŒéBERT
- æ¸›å°‘è³‡æ–™é‡ï¼šç”¨æ–¼å¿«é€Ÿæ¸¬è©¦

### Q2: APIæœå‹™å•Ÿå‹•å¤±æ•—ï¼Ÿ
**A**:
- æª¢æŸ¥åŸ è™Ÿæ˜¯å¦è¢«ä½”ç”¨ï¼š`netstat -an | grep 8000`
- ç¢ºèªæ¨¡å‹æª”æ¡ˆå­˜åœ¨ï¼šæª¢æŸ¥`experiments/models/`ç›®éŒ„
- æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒï¼šæª¢æŸ¥çµ‚ç«¯è¼¸å‡º
- ç¢ºèªä¾è³´å®‰è£ï¼š`pip list | grep fastapi`

### Q3: è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤ï¼Ÿ
**A**:
- æ¸›å°‘batch_size
- ä½¿ç”¨gradient checkpointing
- æ¸…ç†GPUè¨˜æ†¶é«”ï¼š`torch.cuda.empty_cache()`
- ä½¿ç”¨CPUæ¨¡å¼ï¼šè¨­ç½®device='cpu'

### Q4: æ¨¡å‹æº–ç¢ºç‡ä½ï¼Ÿ
**A**:
- å¢åŠ è¨“ç·´epochæ•¸
- èª¿æ•´å­¸ç¿’ç‡
- ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
- æª¢æŸ¥è³‡æ–™å“è³ªå’Œé è™•ç†

### Q5: å¦‚ä½•æ·»åŠ æ–°çš„æ¨¡å‹ï¼Ÿ
**A**:
```python
# 1. åœ¨src/models/ä¸­å®šç¾©æ–°æ¨¡å‹
class CustomModel(nn.Module):
    # æ¨¡å‹å¯¦ä½œ

# 2. åœ¨ModelRegistryä¸­è¨»å†Š
registry.register_model(
    name="custom_model",
    model_path="path/to/model",
    model_type="pytorch"
)

# 3. æ›´æ–°é æ¸¬å™¨æ”¯æ´
```

### Q6: å¦‚ä½•éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒï¼Ÿ
**A**:
```bash
# ä½¿ç”¨Docker
docker build -t sentiment-api .
docker run -p 8000:8000 sentiment-api

# ä½¿ç”¨Gunicorn
pip install gunicorn
gunicorn -w 4 -k uvicorn.workers.UvicornWorker src.inference.api:app

# ä½¿ç”¨Nginxåå‘ä»£ç†
# é…ç½®nginx.confæŒ‡å‘FastAPIæœå‹™
```

## ğŸ“§ ç²å–æ”¯æ´

å¦‚æœé‡åˆ°å•é¡Œï¼š

1. æŸ¥çœ‹[GitHub Issues](issues)
2. æª¢æŸ¥æ—¥èªŒæª”æ¡ˆ
3. æä¾›å®Œæ•´çš„éŒ¯èª¤ä¿¡æ¯
4. æè¿°é‡ç¾æ­¥é©Ÿ

---

**âœ¨ ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼**